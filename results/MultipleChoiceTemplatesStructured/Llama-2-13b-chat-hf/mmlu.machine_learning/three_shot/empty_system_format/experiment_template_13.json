{"card": "cards.mmlu.machine_learning", "template_name": "template_13", "model_name": "meta-llama/Llama-2-13b-chat-hf", "system_format": "empty_system_format", "max_instances": 14000, "num_demos": 3, "demos_pool_size": 20, "results": {"test": [{"Index": 0, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative. Choices: A. True, True or B. False, False or C. True, False or D. False, True Answer:\n", "Result": "C. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The bias-variance tradeoff refers to the relationship between Choices: A. Training error and generalization error or B. Training", "MaxTokenProbability": "C", "Perplexity": 6.9, "GroundTruth": "D. False, True"}, {"Index": 1, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions. Choices: A. True, False or B. True, True or C. False, True or D. False, False Answer:\n", "Result": "B. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good hypothesis? Choices: A. It must be testable or B. It must", "MaxTokenProbability": "B", "Perplexity": 3.31, "GroundTruth": "A. True, False"}, {"Index": 2, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels. Choices: A. False, False or B. True, True or C. False, True or D. True, False Answer:\n", "Result": "C. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good machine learning algorithm? Choices: A. It should be able to learn from a", "MaxTokenProbability": "C", "Perplexity": 4.17, "GroundTruth": "A. False, False"}, {"Index": 3, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there? Choices: A. 72 or B. 12 or C. 24 or D. 48 Answer:\n", "Result": "B. 12\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good machine learning algorithm? Choices: A. It should be easy to implement or B", "MaxTokenProbability": "B", "Perplexity": 4.82, "GroundTruth": "A. 72"}, {"Index": 4, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: As of 2020, which architecture is best for classifying high-resolution images? Choices: A. convolutional networks or B. graph networks or C. RBF networks or D. fully connected networks Answer:\n", "Result": "A. convolutional networks\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The activation function used in the output layer of a neural network is typically chosen to be ___________ Choices: A. sigm", "MaxTokenProbability": "A", "Perplexity": 6.33, "GroundTruth": "A. convolutional networks"}, {"Index": 5, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment. Choices: A. False, True or B. False, False or C. True, False or D. True, True Answer:\n", "Result": "D. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good hypothesis in a hypothesis testing framework. Choices: A. It should be simple and", "MaxTokenProbability": "D", "Perplexity": 4.65, "GroundTruth": "B. False, False"}, {"Index": 6, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient? Choices: A. O(N) or B. O(D) or C. O(ND) or D. O(ND^2) Answer:\n", "Result": "B. O(D)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network compared to a decision tree? Choices: A. Ne", "MaxTokenProbability": "B", "Perplexity": 5.29, "GroundTruth": "B. O(D)"}, {"Index": 7, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 \u2264 p(x) \u2264 1 for all x. Statement 2| Decision tree is learned by minimizing information gain. Choices: A. True, False or B. False, True or C. True, True or D. False, False Answer:\n", "Result": "C. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good feature selection method? Choices: A. It should select all the relevant features or", "MaxTokenProbability": "C", "Perplexity": 6.21, "GroundTruth": "D. False, False"}, {"Index": 8, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W? Choices: A. 4 or B. 2 or C. 8 or D. 16 Answer:\n", "Result": "B. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is not a type of regularization? Choices: A. L1 regularization or B. L2 regularization", "MaxTokenProbability": "B", "Perplexity": 8.14, "GroundTruth": "C. 8"}, {"Index": 9, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: As the number of training examples goes to infinity, your model trained on that data will have: Choices: A. None of the above or B. Higher variance or C. Same variance or D. Lower variance Answer:\n", "Result": "D. Lower variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the purpose of regularization in machine learning? Choices: A. To reduce overfitting or B. To increase underf", "MaxTokenProbability": "D", "Perplexity": 5.37, "GroundTruth": "D. Lower variance"}, {"Index": 10, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The set of all rectangles in the 2D plane (which includes non axisaligned rectangles) can shatter a set of 5 points. Statement 2| The VC-dimension of k-Nearest Neighbour classifier when k = 1 is infinite. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\n", "Result": "C. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Consider a binary classification problem where the feature set consists of two features X1 and X2. The decision boundary is a hyperplane that", "MaxTokenProbability": "C", "Perplexity": 8.26, "GroundTruth": "C. True, True"}, {"Index": 11, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: _ refers to a model that can neither model the training data nor generalize to new data. Choices: A. overfitting or B. underfitting or C. all of the above or D. good fitting Answer:\n", "Result": "B. underfitting\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good model? Choices: A. high bias or B. high variance or C.", "MaxTokenProbability": "B", "Perplexity": 3.64, "GroundTruth": "B. underfitting"}, {"Index": 12, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The F1 score can be especially useful for datasets with class high imbalance. Statement 2| The area under the ROC curve is one of the main metrics used to assess anomaly detectors. Choices: A. True, True or B. False, True or C. False, False or D. True, False Answer:\n", "Result": "B. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The bias-variance tradeoff is a fundamental property of machine learning models. Choices: A. True or B. False Answer", "MaxTokenProbability": "B", "Perplexity": 5.12, "GroundTruth": "A. True, True"}, {"Index": 13, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The back-propagation algorithm learns a globally optimal neural network with hidden layers. Statement 2| The VC dimension of a line should be at most 2, since I can find at least one case of 3 points that cannot be shattered by any line. Choices: A. True, True or B. True, False or C. False, False or D. False, True Answer:\n", "Result": "B. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the difference between a neural network with one hidden layer and a linear regression model? Choices: A. A neural network with", "MaxTokenProbability": "B", "Perplexity": 7.08, "GroundTruth": "C. False, False"}, {"Index": 14, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: High entropy means that the partitions in classification are Choices: A. useful or B. not pure or C. useless or D. pure Answer:\n", "Result": "B. not pure\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a decision tree as a classifier? Choices: A. It is prone", "MaxTokenProbability": "B", "Perplexity": 5.21, "GroundTruth": "B. not pure"}, {"Index": 15, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training. Choices: A. True, False or B. False, False or C. True, True or D. False, True Answer:\n", "Result": "C. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network with a large number of layers? Choices: A. It", "MaxTokenProbability": "C", "Perplexity": 4.01, "GroundTruth": "B. False, False"}, {"Index": 16, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In building a linear regression model for a particular data set, you observe the coefficient of one of the features having a relatively high negative value. This suggests that Choices: A. Nothing can be determined. or B. It is not possible to comment on the importance of this feature without additional information or C. This feature has a strong effect on the model (should be retained) or D. This feature does not have a strong effect on the model (should be ignored) Answer:\n", "Result": "C. This feature has a strong effect on the model (should be retained)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good model? Choices: A", "MaxTokenProbability": "C", "Perplexity": 4.7, "GroundTruth": "B. It is not possible to comment on the importance of this feature without additional information"}, {"Index": 17, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For a neural network, which one of these structural assumptions is the one that most affects the trade-off between underfitting (i.e. a high bias model) and overfitting (i.e. a high variance model): Choices: A. The number of hidden nodes or B. The initial choice of weights or C. The learning rate or D. The use of a constant-term unit input Answer:\n", "Result": "C. The learning rate\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good regularization term for a linear regression model: Choices: A. It should be", "MaxTokenProbability": "C", "Perplexity": 6.63, "GroundTruth": "A. The number of hidden nodes"}, {"Index": 18, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For polynomial regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting: Choices: A. The use of a constant-term unit input or B. The polynomial degree or C. The assumed variance of the Gaussian noise or D. Whether we learn the weights by matrix inversion or gradient descent Answer:\n", "Result": "B. The polynomial degree\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good hypothesis test: Choices: A. The test statistic is calculated using the sample", "MaxTokenProbability": "B", "Perplexity": 3.95, "GroundTruth": "B. The polynomial degree"}, {"Index": 19, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| As of 2020, some models attain greater than 98% accuracy on CIFAR-10. Statement 2| The original ResNets were not optimized with the Adam optimizer. Choices: A. True, True or B. False, False or C. False, True or D. True, False Answer:\n", "Result": "C. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the difference between a convolutional neural network (CNN) and a recurrent neural network (RNN)? Choices:", "MaxTokenProbability": "C", "Perplexity": 6.13, "GroundTruth": "A. True, True"}, {"Index": 20, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The K-means algorithm: Choices: A. Requires the dimension of the feature space to be no bigger than the number of samples or B. Minimizes the within class variance for a given number of clusters or C. Converges to the global optimum if and only if the initial means are chosen as some of the samples themselves or D. Has the smallest value of the objective function when K = 1 Answer:\n", "Result": "B. Minimizes the within class variance for a given number of clusters\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The difference between the Bayesian and classical approaches to statistical inference is that Choices:", "MaxTokenProbability": "B", "Perplexity": 3.35, "GroundTruth": "B. Minimizes the within class variance for a given number of clusters"}, {"Index": 21, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| VGGNets have convolutional kernels of smaller width and height than AlexNet's first-layer kernels. Statement 2| Data-dependent weight initialization procedures were introduced before Batch Normalization. Choices: A. False, False or B. False, True or C. True, False or D. True, True Answer:\n", "Result": "D. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a large number of hidden layers in a neural network? Choices: A.", "MaxTokenProbability": "D", "Perplexity": 5.39, "GroundTruth": "D. True, True"}, {"Index": 22, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the rank of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 0 or B. 1 or C. 2 or D. 3 Answer:\n", "Result": "C. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is NOT a type of bias in machine learning? Choices: A. sampling bias or B. data bias or C", "MaxTokenProbability": "C", "Perplexity": 2.87, "GroundTruth": "B. 1"}, {"Index": 23, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Density estimation (using say, the kernel density estimator) can be used to perform classification. Statement 2| The correspondence between logistic regression and Gaussian Naive Bayes (with identity class covariances) means that there is a one-to-one correspondence between the parameters of the two classifiers. Choices: A. False, True or B. True, False or C. False, False or D. True, True Answer:\n", "Result": "B. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a valid assumption for the use of linear regression in modeling the relationship between a dependent variable and one or more", "MaxTokenProbability": "B", "Perplexity": 4.08, "GroundTruth": "B. True, False"}, {"Index": 24, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Suppose we would like to perform clustering on spatial data such as the geometrical locations of houses. We wish to produce clusters of many different sizes and shapes. Which of the following methods is the most appropriate? Choices: A. K-means clustering or B. Density-based clustering or C. Decision Trees or D. Model-based clustering Answer:\n", "Result": "B. Density-based clustering\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network with a large number of layers? Choices", "MaxTokenProbability": "B", "Perplexity": 5.88, "GroundTruth": "B. Density-based clustering"}, {"Index": 25, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| In AdaBoost weights of the misclassified examples go up by the same multiplicative factor. Statement 2| In AdaBoost, weighted training error e_t of the tth weak classifier on training data with weights D_t tends to increase as a function of t. Choices: A. True, True or B. False, True or C. False, False or D. True, False Answer:\n", "Result": "C. False, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using Naive Bayes? Choices: A. It assumes that all features are", "MaxTokenProbability": "C", "Perplexity": 3.53, "GroundTruth": "A. True, True"}, {"Index": 26, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: MLE estimates are often undesirable because Choices: A. None of the above or B. they are biased or C. they have high variance or D. they are not consistent estimators Answer:\n", "Result": "C. they have high variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network for regression? Choices: A. They are computationally", "MaxTokenProbability": "C", "Perplexity": 5.13, "GroundTruth": "C. they have high variance"}, {"Index": 27, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Computational complexity of Gradient descent is, Choices: A. linear in D or B. polynomial in D or C. linear in N or D. dependent on the number of iterations Answer:\n", "Result": "B. polynomial in D\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network? Choices: A. They are computationally expensive or B", "MaxTokenProbability": "B", "Perplexity": 4.63, "GroundTruth": "B. polynomial in D"}, {"Index": 28, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Averaging the output of multiple decision trees helps _. Choices: A. Increase bias or B. Increase variance or C. Decrease bias or D. Decrease variance Answer:\n", "Result": "C. Decrease bias.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using Naive Bayes? Choices: A. It is computationally", "MaxTokenProbability": "C", "Perplexity": 3.02, "GroundTruth": "D. Decrease variance"}, {"Index": 29, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The model obtained by applying linear regression on the identified subset of features may differ from the model obtained at the end of the process of identifying the subset during Choices: A. Best-subset selection or B. All of the above or C. Forward stepwise selection or D. Forward stage wise selection Answer:\n", "Result": "C. Forward stepwise selection\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is not a characteristic of a good feature selection method? Choices: A. High accuracy or B.", "MaxTokenProbability": "C", "Perplexity": 4.37, "GroundTruth": "D. Forward stage wise selection"}, {"Index": 30, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Neural networks: Choices: A. None of the above or B. Can only be trained with stochastic gradient descent or C. Optimize a convex objective function or D. Can use a mix of different activation functions Answer:\n", "Result": "B. Can only be trained with stochastic gradient descent\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The Bayes risk is Choices: A. The expected value of the risk under the null hypothesis or", "MaxTokenProbability": "B", "Perplexity": 3.31, "GroundTruth": "D. Can use a mix of different activation functions"}, {"Index": 31, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient \u201chas disease D\u201d and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(TP), the prior probability of testing positive. Choices: A. 0.0368 or B. 0.078 or C. 0.473 or D. None of the above Answer:\n", "Result": "B. 0.078\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good neural network architecture? Choices: A. It should have a large", "MaxTokenProbability": "B", "Perplexity": 5.1, "GroundTruth": "B. 0.078"}, {"Index": 32, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can\u2019t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM. Choices: A. True, True or B. False, True or C. True, False or D. False, False Answer:\n", "Result": "C. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The bias term in a linear regression model can be interpreted as the Choices: A. Average value of the dependent variable. or", "MaxTokenProbability": "C", "Perplexity": 7.09, "GroundTruth": "D. False, False"}, {"Index": 33, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The disadvantage of Grid search is Choices: A. It is hard to implement. or B. It can not be applied to non-continuous functions. or C. It can not be applied to non-differentiable functions. or D. It runs reasonably slow for multiple linear regression. Answer:\n", "Result": "C. It can not be applied to non-differentiable functions.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The advantage of Random Forest over Decision Trees is Choices: A.", "MaxTokenProbability": "C", "Perplexity": 5.12, "GroundTruth": "D. It runs reasonably slow for multiple linear regression."}, {"Index": 34, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Predicting the amount of rainfall in a region based on various cues is a ______ problem. Choices: A. Unsupervised learning or B. Clustering or C. None of the above or D. Supervised learning Answer:\n", "Result": "D. Supervised learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The bias-variance tradeoff is a fundamental property of all machine learning algorithms, and it is characterized by the following two extrem", "MaxTokenProbability": "D", "Perplexity": 7.18, "GroundTruth": "D. Supervised learning"}, {"Index": 35, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following sentence is FALSE regarding regression? Choices: A. It relates inputs to outputs. or B. It is used for prediction. or C. It may be used for interpretation. or D. It discovers causal relationships Answer:\n", "Result": "C. It may be used for interpretation.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a decision tree as a classifier? Choices: A", "MaxTokenProbability": "C", "Perplexity": 4.37, "GroundTruth": "D. It discovers causal relationships"}, {"Index": 36, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which one of the following is the main reason for pruning a Decision Tree? Choices: A. To avoid overfitting the training set or B. To save space for storing the Decision Tree or C. To save computing time during testing or D. To make the training set error smaller Answer:\n", "Result": "A. To avoid overfitting the training set.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using Naive Bayes? Choices: A.", "MaxTokenProbability": "A", "Perplexity": 4.23, "GroundTruth": "A. To avoid overfitting the training set"}, {"Index": 37, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The kernel density estimator is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set. Statement 2| The depth of a learned decision tree can be larger than the number of training examples used to create the tree. Choices: A. True, False or B. False, True or C. False, False or D. True, True Answer:\n", "Result": "B. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The bias-variance tradeoff refers to the relationship between Choices: A. the complexity of a model and its ability to fit", "MaxTokenProbability": "B", "Perplexity": 7.68, "GroundTruth": "C. False, False"}, {"Index": 38, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Suppose your model is overfitting. Which of the following is NOT a valid way to try and reduce the overfitting? Choices: A. Increase the amount of training data. or B. Reduce the noise in the training data. or C. Improve the optimisation algorithm being used for error minimisation. or D. Decrease the model complexity. Answer:\n", "Result": "A. Increase the amount of training data.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network compared to a linear regression model?", "MaxTokenProbability": "A", "Perplexity": 6.8, "GroundTruth": "C. Improve the optimisation algorithm being used for error minimisation."}, {"Index": 39, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The softmax function is commonly used in mutliclass logistic regression. Statement 2| The temperature of a nonuniform softmax distribution affects its entropy. Choices: A. False, False or B. False, True or C. True, True or D. True, False Answer:\n", "Result": "C. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network with a large number of hidden units? Choices: A.", "MaxTokenProbability": "C", "Perplexity": 7.74, "GroundTruth": "C. True, True"}, {"Index": 40, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is/are true regarding an SVM? Choices: A. In theory, a Gaussian kernel SVM cannot model any complex separating hyperplane. or B. For every kernel function used in a SVM, one can obtain an equivalent closed form basis expansion. or C. Overfitting in an SVM is not a function of number of support vectors. or D. For two dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line. Answer:\n", "Result": "B. For every kernel function used in a SVM, one can obtain an equivalent closed form basis expansion.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is/are true regarding the", "MaxTokenProbability": "B", "Perplexity": 7.46, "GroundTruth": "D. For two dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line."}, {"Index": 41, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is the joint probability of H, U, P, and W described by the given Bayesian Network H -> U <- P <- W? [note: as the product of the conditional probabilities] Choices: A. None of the above or B. P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P) or C. P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P) or D. P(H, U, P, W) = P(H) * P(W) * P(P) * P(U) Answer:\n", "Result": "B. P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)\n\nQuestion: [question] Choices: [choices] Answer:", "MaxTokenProbability": "B", "Perplexity": 4.04, "GroundTruth": "B. P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)"}, {"Index": 42, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Since the VC dimension for an SVM with a Radial Base Kernel is infinite, such an SVM must be worse than an SVM with polynomial kernel which has a finite VC dimension. Statement 2| A two layer neural network with linear activation functions is essentially a weighted combination of linear separators, trained on a given dataset; the boosting algorithm built on linear separators also finds a combination of linear separators, therefore these two algorithms will give the same result. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\n", "Result": "C. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network with a large number of hidden units. Choices: A.", "MaxTokenProbability": "C", "Perplexity": 6.41, "GroundTruth": "D. False, False"}, {"Index": 43, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The ID3 algorithm is guaranteed to find the optimal decision tree. Statement 2| Consider a continuous probability distribution with density f() that is nonzero everywhere. The probability of a value x is equal to f(x). Choices: A. True, True or B. False, False or C. True, False or D. False, True Answer:\n", "Result": "C. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a characteristic of a good machine learning algorithm? Choices: A. It must be simple to implement or B", "MaxTokenProbability": "C", "Perplexity": 3.47, "GroundTruth": "B. False, False"}, {"Index": 44, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Given a Neural Net with N input nodes, no hidden layers, one output node, with Entropy Loss and Sigmoid Activation Functions, which of the following algorithms (with the proper hyper-parameters and initialization) can be used to find the global optimum? Choices: A. Stochastic Gradient Descent or B. All of the above or C. Batch Gradient Descent or D. Mini-Batch Gradient Descent Answer:\n", "Result": "B. All of the above.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a Neural Network for Regression? Choices: A. They", "MaxTokenProbability": "B", "Perplexity": 6.2, "GroundTruth": "B. All of the above"}, {"Index": 45, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Adding more basis functions in a linear model, pick the most probably option: Choices: A. Decreases model bias or B. Decreases estimation bias or C. Decreases variance or D. Doesn\u2019t affect bias and variance Answer:\n", "Result": "B. Decreases estimation bias\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The bias-variance tradeoff in linear regression is primarily determined by: Choices: A. The number of basis functions", "MaxTokenProbability": "B", "Perplexity": 5.54, "GroundTruth": "A. Decreases model bias"}, {"Index": 46, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Consider the Bayesian network given below. How many independent parameters would we need if we made no assumptions about independence or conditional independence H -> U <- P <- W? Choices: A. 4 or B. 15 or C. 7 or D. 3 Answer:\n", "Result": "B. 15\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a valid reason for using a regularization term in a linear regression model? Choices: A. To reduce", "MaxTokenProbability": "B", "Perplexity": 3.86, "GroundTruth": "B. 15"}, {"Index": 47, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Another term for out-of-distribution detection is? Choices: A. anomaly detection or B. train-test mismatch robustness or C. background detection or D. one-class detection Answer:\n", "Result": "D. one-class detection\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the purpose of the \"dropout\" regularization technique in deep learning? Choices: A. To prevent overf", "MaxTokenProbability": "D", "Perplexity": 5.18, "GroundTruth": "A. anomaly detection"}, {"Index": 48, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| We learn a classifier f by boosting weak learners h. The functional form of f\u2019s decision boundary is the same as h\u2019s, but with different parameters. (e.g., if h was a linear classifier, then f is also a linear classifier). Statement 2| Cross validation can be used to select the number of iterations in boosting; this procedure may help reduce overfitting. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\n", "Result": "B. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a decision tree as a classifier? Choices: A. It is sensitive", "MaxTokenProbability": "B", "Perplexity": 5.44, "GroundTruth": "A. False, True"}, {"Index": 49, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Highway networks were introduced after ResNets and eschew max pooling in favor of convolutions. Statement 2| DenseNets usually cost more memory than ResNets. Choices: A. True, True or B. False, False or C. False, True or D. True, False Answer:\n", "Result": "C. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the main difference between a Bayesian linear regression model and a classical linear regression model? Choices: A. The prior distribution", "MaxTokenProbability": "C", "Perplexity": 4.06, "GroundTruth": "C. False, True"}, {"Index": 50, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: A. supervised learning or B. self-supervised learning or C. reinforcement learning or D. unsupervised learning Answer:\nC. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: If N is the number of instances in the training dataset, nearest neighbors has a classification run time of Choices: A. O(1) or B. O( N^2 ) or C. O(log N ) or D. O( N ) Answer:\n", "Result": "B. O( N^2 )\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The algorithm that is guaranteed to find the global minimum of a nonlinear function is Choices: A. gradient descent or", "MaxTokenProbability": "B", "Perplexity": 7.81, "GroundTruth": "D. O( N )"}, {"Index": 51, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The original ResNets and Transformers are feedforward neural networks. Statement 2| The original Transformers use self-attention, but the original ResNet does not. Choices: A. True, False or B. True, True or C. False, True or D. False, False Answer:\n", "Result": "B. True, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a large learning rate in training a neural network? Choices: A. It", "MaxTokenProbability": "B", "Perplexity": 4.82, "GroundTruth": "B. True, True"}, {"Index": 52, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| RELUs are not monotonic, but sigmoids are monotonic. Statement 2| Neural networks trained with gradient descent with high probability converge to the global optimum. Choices: A. False, False or B. True, True or C. True, False or D. False, True Answer:\n", "Result": "C. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Consider the following statement: \"The optimal learning rate for a neural network is the one that minimizes the training error.\" Choose the", "MaxTokenProbability": "C", "Perplexity": 7.3, "GroundTruth": "D. False, True"}, {"Index": 53, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: A. Attributes are equally important. or B. Attributes can be nominal or numeric or C. Attributes are statistically independent of one another given the class value. or D. Attributes are statistically dependent of one another given the class value. Answer:\nD. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The numerical output of a sigmoid node in a neural network: Choices: A. Is unbounded, encompassing all integers. or B. Is unbounded, encompassing all real numbers. or C. Is bounded between 0 and 1. or D. Is bounded between -1 and 1. Answer:\n", "Result": "C. Is bounded between 0 and 1.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a limitation of linear regression? Choices: A. It assumes a linear relationship", "MaxTokenProbability": "C", "Perplexity": 3.49, "GroundTruth": "C. Is bounded between 0 and 1."}, {"Index": 54, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\nA. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following can only be used when training data are linearly separable? Choices: A. The centroid method. or B. Linear hard-margin SVM. or C. Linear Logistic Regression. or D. Linear Soft margin SVM. Answer:\n", "Result": "B. Linear hard-margin SVM.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the purpose of the regularization term in a linear regression model? Choices: A. To reduce over", "MaxTokenProbability": "B", "Perplexity": 5.87, "GroundTruth": "B. Linear hard-margin SVM."}, {"Index": 55, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: A. 3 or B. 0 or C. 1 or D. 2 Answer:\nD. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following are the spatial clustering algorithms? Choices: A. K-means clustering or B. Partitioning based clustering or C. All of the above or D. Grid based clustering Answer:\n", "Result": "C. All of the above\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the purpose of the \"Holdout\" method in cross-validation? Choices: A. To evaluate the model'", "MaxTokenProbability": "C", "Perplexity": 2.93, "GroundTruth": "C. All of the above"}, {"Index": 56, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: A. Should not set it to zero since otherwise it will cause overfitting or B. Should set it to zero in order to preserve symmetry across all neurons or C. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space or D. Should set it to zero since otherwise it causes a bias Answer:\nC. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: A. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y) or B. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y) or C. P(X, Y, Z) = P(X) * P(Y) * P(Z) or D. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z) Answer:\nB. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: A. High estimation bias or B. High model bias or C. High variance or D. None of the above Answer:\nC. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The maximum margin decision boundaries that support vector machines construct have the lowest generalization error among all linear classifiers. Statement 2| Any decision boundary that we get from a generative model with classconditional Gaussian distributions could in principle be reproduced with an SVM and a polynomial kernel of degree less than or equal to three. Choices: A. True, True or B. False, True or C. True, False or D. False, False Answer:\n", "Result": "B. False, True\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following is a disadvantage of using a neural network compared to a linear regression model? Choices: A. Ne", "MaxTokenProbability": "B", "Perplexity": 4.9, "GroundTruth": "B. False, True"}, {"Index": 57, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: A. none of the above or B. scaling MSE by half makes gradient descent converge faster. or C. presence of half makes it easy to do grid search.  or D. it does not matter whether half is there or not.  Answer:\nD. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: A. Larger if the error rate is smaller. or B. Smaller if the error rate is smaller. or C. Larger if the error rate is larger. or D. It does not matter. Answer:\nA. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: A. lower or B. same or C. it could be any of the above or D. higher Answer:\nA. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| L2 regularization of linear models tends to make models more sparse than L1 regularization. Statement 2| Residual connections can be found in ResNets and Transformers. Choices: A. False, True or B. True, False or C. True, True or D. False, False Answer:\n", "Result": "B. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The bias term in a linear model is ________ Choices: A. a constant or B. a function of the input or", "MaxTokenProbability": "B", "Perplexity": 6.8, "GroundTruth": "A. False, True"}]}}