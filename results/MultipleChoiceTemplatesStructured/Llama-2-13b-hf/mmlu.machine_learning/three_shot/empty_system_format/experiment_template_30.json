{"card": "cards.mmlu.machine_learning", "template_name": "template_30", "model_name": "meta-llama/Llama-2-13b-hf", "system_format": "empty_system_format", "max_instances": 1600, "num_demos": 3, "demos_pool_size": 20, "results": {"test": [{"Index": 0, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: 1. Larger if the error rate is larger.\n2. Larger if the error rate is smaller.\n3. Smaller if the error rate is smaller.\n4. It does not matter. Answer:\n2. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: 1. 0\n2. 1\n3. 2\n4. 3 Answer:\n3. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: 1. higher\n2. same\n3. lower\n4. it could be any of the above Answer:\n3. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "3. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\n", "MaxTokenProbability": "3", "GroundTruth": "4. False, True", "Score": 0.0}, {"Index": 1, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: 1. higher\n2. same\n3. lower\n4. it could be any of the above Answer:\n3. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: 1. Should not set it to zero since otherwise it will cause overfitting\n2. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n3. Should set it to zero since otherwise it causes a bias\n4. Should set it to zero in order to preserve symmetry across all neurons Answer:\n2. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: 1. scaling MSE by half makes gradient descent converge faster.\n2. presence of half makes it easy to do grid search. \n3. it does not matter whether half is there or not. \n4. none of the above Answer:\n3. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "3. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\n", "MaxTokenProbability": "3", "GroundTruth": "3. True, False", "Score": 1.0}, {"Index": 2, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: 1. 0\n2. 1\n3. 2\n4. 3 Answer:\n3. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option. Choices: 1. High variance\n2. High model bias\n3. High estimation bias\n4. None of the above Answer:\n1. High variance\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network. Choices: 1. Should not set it to zero since otherwise it will cause overfitting\n2. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n3. Should set it to zero since otherwise it causes a bias\n4. Should set it to zero in order to preserve symmetry across all neurons Answer:\n2. Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "3. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\n", "MaxTokenProbability": "3", "GroundTruth": "2. False, False", "Score": 0.0}, {"Index": 3, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: 1. Larger if the error rate is larger.\n2. Larger if the error rate is smaller.\n3. Smaller if the error rate is smaller.\n4. It does not matter. Answer:\n2. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: 1. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n2. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y)\n3. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z)\n4. P(X, Y, Z) = P(X) * P(Y) * P(Z) Answer:\n1. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: 1. Attributes are equally important.\n2. Attributes are statistically dependent of one another given the class value.\n3. Attributes are statistically independent of one another given the class value.\n4. Attributes can be nominal or numeric Answer:\n2. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there? Choices: 1. 12\n2. 24\n3. 48\n4. 72 Answer:\n", "Result": "2. 24\n\nQuestion: [question] Choices: [choices] Answer: [answer]\n", "MaxTokenProbability": "2", "GroundTruth": "4. 72", "Score": 0.0}, {"Index": 4, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: 1. reinforcement learning\n2. self-supervised learning\n3. unsupervised learning\n4. supervised learning Answer:\n1. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________ Choices: 1. higher\n2. same\n3. lower\n4. it could be any of the above Answer:\n3. lower\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: 1. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n2. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y)\n3. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z)\n4. P(X, Y, Z) = P(X) * P(Y) * P(Z) Answer:\n1. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: As of 2020, which architecture is best for classifying high-resolution images? Choices: 1. convolutional networks\n2. graph networks\n3. fully connected networks\n4. RBF networks Answer:\n", "Result": "1. convolutional networks\n\nQuestion: [question] Choices: [choices] Answer: [answer]\n", "MaxTokenProbability": "1", "GroundTruth": "1. convolutional networks", "Score": 1.0}, {"Index": 5, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: 1. 0\n2. 1\n3. 2\n4. 3 Answer:\n3. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: 1. Larger if the error rate is larger.\n2. Larger if the error rate is smaller.\n3. Smaller if the error rate is smaller.\n4. It does not matter. Answer:\n2. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: 1. Attributes are equally important.\n2. Attributes are statistically dependent of one another given the class value.\n3. Attributes are statistically independent of one another given the class value.\n4. Attributes can be nominal or numeric Answer:\n2. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "3. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\n", "MaxTokenProbability": "3", "GroundTruth": "2. False, False", "Score": 0.0}, {"Index": 6, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: In Yann LeCun's cake, the cherry on top is Choices: 1. reinforcement learning\n2. self-supervised learning\n3. unsupervised learning\n4. supervised learning Answer:\n1. reinforcement learning\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z? Choices: 1. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n2. P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y)\n3. P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z)\n4. P(X, Y, Z) = P(X) * P(Y) * P(Z) Answer:\n1. P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]] Choices: 1. 0\n2. 1\n3. 2\n4. 3 Answer:\n3. 2\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient? Choices: 1. O(D)\n2. O(N)\n3. O(ND)\n4. O(ND^2) Answer:\n", "Result": "3. O(ND)\n\nQuestion: [question] Choices: [choices] Answer: [answer]", "MaxTokenProbability": "3", "GroundTruth": "1. O(D)", "Score": 0.0}, {"Index": 7, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: The number of test examples needed to get statistically significant results should be _ Choices: 1. Larger if the error rate is larger.\n2. Larger if the error rate is smaller.\n3. Smaller if the error rate is smaller.\n4. It does not matter. Answer:\n2. Larger if the error rate is smaller.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \\sum (y_n \u2212 \u03b2_0)^2 . We have a half term in the front because, Choices: 1. scaling MSE by half makes gradient descent converge faster.\n2. presence of half makes it easy to do grid search. \n3. it does not matter whether half is there or not. \n4. none of the above Answer:\n3. it does not matter whether half is there or not. \n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Which of the following statements about Naive Bayes is incorrect? Choices: 1. Attributes are equally important.\n2. Attributes are statistically dependent of one another given the class value.\n3. Attributes are statistically independent of one another given the class value.\n4. Attributes can be nominal or numeric Answer:\n2. Attributes are statistically dependent of one another given the class value.\n\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 \u2264 p(x) \u2264 1 for all x. Statement 2| Decision tree is learned by minimizing information gain. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "3. True, False\n\nQuestion: [question] Choices: [choices] Answer: [answer]\n", "MaxTokenProbability": "3", "GroundTruth": "2. False, False", "Score": 0.0}]}}