{"card": "cards.mmlu.machine_learning", "template_name": "template_48", "model_name": "allenai/OLMo-7B-Instruct-hf", "system_format": "empty_system_format", "max_instances": 1600, "num_demos": 0, "demos_pool_size": 1, "results": {"test": [{"Index": 0, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative. Choices: I. True, True; II. False, False; III. True, False; IV. False, True Answer:\n", "Result": "Question: In a binary classification problem, the value of the risk function R(t) is the probability of a wrong classification", "MaxTokenProbability": null, "GroundTruth": "IV. False, True", "Score": 0.0}, {"Index": 1, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions. Choices: I. True, True; II. False, False; III. True, False; IV. False, True Answer:\n", "Result": "Question: The first commercial aircraft to fly the wing-body concept had a fuselage length of 15 m and a wings", "MaxTokenProbability": null, "GroundTruth": "III. True, False", "Score": 1.0}, {"Index": 2, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels. Choices: I. True, True; II. False, False; III. True, False; IV. False, True Answer:\n", "Result": "Question: Which of the following is a valid Python string format for a date in the format 'YYYY-MM-DD", "MaxTokenProbability": null, "GroundTruth": "II. False, False", "Score": 0.0}, {"Index": 3, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there? Choices: I. 12; II. 24; III. 48; IV. 72 Answer:\n", "Result": "The answer is: III. 48;\nExplanation: There are 8 combinations (per attribute) of class values because 3", "MaxTokenProbability": null, "GroundTruth": "IV. 72", "Score": 0.0}, {"Index": 4, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: As of 2020, which architecture is best for classifying high-resolution images? Choices: I. convolutional networks; II. graph networks; III. fully connected networks; IV. RBF networks Answer:\n", "Result": "A. I. convolutional networks\nB. II. graph networks\nThe correct answer is A. I. convolutional", "MaxTokenProbability": null, "GroundTruth": "I. convolutional networks", "Score": 1.0}, {"Index": 5, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment. Choices: I. True, True; II. False, False; III. True, False; IV. False, True Answer:\n", "Result": "IV. False, True", "MaxTokenProbability": null, "GroundTruth": "II. False, False", "Score": 0.0}, {"Index": 6, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient? Choices: I. O(D); II. O(N); III. O(ND); IV. O(ND^2) Answer:\n", "Result": "[Answer: I. O(D)]\nExplanation: The cost of one gradient descent update is given by the negative", "MaxTokenProbability": null, "GroundTruth": "I. O(D)", "Score": 1.0}, {"Index": 7, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 \u2264 p(x) \u2264 1 for all x. Statement 2| Decision tree is learned by minimizing information gain. Choices: I. True, True; II. False, False; III. True, False; IV. False, True Answer:\n", "Result": "Question: What is the difference between a primary key and a foreign key in a relational database? Choices: A. The", "MaxTokenProbability": null, "GroundTruth": "II. False, False", "Score": 0.0}, {"Index": 8, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W? Choices: I. 2; II. 4; III. 8; IV. 16 Answer:\n", "Result": "[Answer: III. 8]\nExplanation: The number of independent parameters is 8, since H is directly dependent on", "MaxTokenProbability": null, "GroundTruth": "III. 8", "Score": 1.0}, {"Index": 9, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: As the number of training examples goes to infinity, your model trained on that data will have: Choices: I. Lower variance; II. Higher variance; III. Same variance; IV. None of the above Answer:\n", "Result": "Question: In machine learning, the concept of regularization is used to: Choices: A. Prevent overfitting; B.", "MaxTokenProbability": null, "GroundTruth": "I. Lower variance", "Score": 0.0}, {"Index": 10, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The set of all rectangles in the 2D plane (which includes non axisaligned rectangles) can shatter a set of 5 points. Statement 2| The VC-dimension of k-Nearest Neighbour classifier when k = 1 is infinite. Choices: I. True, True; II. False, False; III. True, False; IV. False, True Answer:\n", "Result": "Answer: III. True, False", "MaxTokenProbability": null, "GroundTruth": "I. True, True", "Score": 0.0}, {"Index": 11, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: _ refers to a model that can neither model the training data nor generalize to new data. Choices: I. good fitting; II. overfitting; III. underfitting; IV. all of the above Answer:\n", "Result": "IV. all of the above", "MaxTokenProbability": null, "GroundTruth": "III. underfitting", "Score": 0.0}, {"Index": 12, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The F1 score can be especially useful for datasets with class high imbalance. Statement 2| The area under the ROC curve is one of the main metrics used to assess anomaly detectors. Choices: I. True, True; II. False, False; III. True, False; IV. False, True Answer:\n", "Result": "Question: Which of the following is a type of machine learning algorithm used for dimensionality reduction? [A] Principal Component Analysis", "MaxTokenProbability": null, "GroundTruth": "I. True, True", "Score": 0.0}, {"Index": 13, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The back-propagation algorithm learns a globally optimal neural network with hidden layers. Statement 2| The VC dimension of a line should be at most 2, since I can find at least one case of 3 points that cannot be shattered by any line. Choices: I. True, True; II. False, False; III. True, False; IV. False, True Answer:\n", "Result": "Question: Which of the following sentences is nonsensical? Select one: I. \"The human eye can see the entire", "MaxTokenProbability": null, "GroundTruth": "II. False, False", "Score": 0.0}, {"Index": 14, "Instance": "Question: [question] Choices: [choices] Answer: [answer]\nQuestion: High entropy means that the partitions in classification are Choices: I. pure; II. not pure; III. useful; IV. useless Answer:\n", "Result": "\nI. pure\nQuestion: The term \"entropy\" in information theory was first introduced by Answer: [author]", "MaxTokenProbability": null, "GroundTruth": "II. not pure", "Score": 0.0}]}}