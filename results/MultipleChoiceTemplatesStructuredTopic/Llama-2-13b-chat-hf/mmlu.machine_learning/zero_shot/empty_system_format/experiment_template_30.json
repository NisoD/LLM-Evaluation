{"card": "cards.mmlu.machine_learning", "template_name": "template_30", "model_name": "meta-llama/Llama-2-13b-chat-hf", "system_format": "empty_system_format", "max_instances": 14000, "num_demos": 0, "demos_pool_size": 10, "results": {"test": [{"Index": 0, "Instance": "Topic: machine learning\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "\nAnswer: 3. True, False\n\nExplanation:\n\nStatement 1 is True. The linear", "MaxTokenProbability": "3", "Perplexity": 17.71, "GroundTruth": "4. False, True"}, {"Index": 1, "Instance": "Topic: machine learning\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "\nTopic: machine learning\nQuestion: [question] Choices: [choices] Answer: [answer]\n", "MaxTokenProbability": "3", "Perplexity": 15.66, "GroundTruth": "3. True, False"}, {"Index": 2, "Instance": "Topic: machine learning\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "\nAnswer: 3. True, False\n\nExplanation:\n\nStatement 1 is True. Support vector", "MaxTokenProbability": "3", "Perplexity": 16.45, "GroundTruth": "2. False, False"}, {"Index": 3, "Instance": "Topic: machine learning\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there? Choices: 1. 12\n2. 24\n3. 48\n4. 72 Answer:\n", "Result": "\nAnswer: 3. 48\n\nExplanation:\n\nThe problem involves four attributes, each with", "MaxTokenProbability": "3", "Perplexity": 10.72, "GroundTruth": "4. 72"}, {"Index": 4, "Instance": "Topic: machine learning\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: As of 2020, which architecture is best for classifying high-resolution images? Choices: 1. convolutional networks\n2. graph networks\n3. fully connected networks\n4. RBF networks Answer:\n", "Result": "1. convolutional networks\n\nQuestion: Why is it difficult to train deep neural networks for image classification tasks? Choices", "MaxTokenProbability": "1", "Perplexity": 23.04, "GroundTruth": "1. convolutional networks"}, {"Index": 5, "Instance": "Topic: machine learning\nQuestion: [question] Choices: [choices] Answer: [answer]\nQuestion: Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment. Choices: 1. True, True\n2. False, False\n3. True, False\n4. False, True Answer:\n", "Result": "\nCorrect answer: 3. True, False\n\nExplanation:\n\nStatement 1 is false.", "MaxTokenProbability": "3", "Perplexity": 15.88, "GroundTruth": "2. False, False"}]}}